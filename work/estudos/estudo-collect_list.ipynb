{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b55f87-0fb7-4cf3-b5e9-150062694eab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/18 12:55:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d5c28177bfc8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Testando</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1318009900>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "if 'spark' in locals() or 'spark' in globals():\n",
    "    spark.stop()\n",
    "    \n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Testando\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb7b218-114b-41f3-bb9c-a343c9821e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, collect_list, concat_ws, current_date, date_add, date_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835a8a43-6c06-4ede-b4d5-e390fadcbdc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+----------------+\n",
      "| empId|firstName|lastName|             job|\n",
      "+------+---------+--------+----------------+\n",
      "|200001|  Michael|   Scott|Regional Manager|\n",
      "|200002|   Dwight| Schrute|           Sales|\n",
      "|200003|      Jim| Halpert|           Sales|\n",
      "|200004|  Phyllis|   Lapin|           Sales|\n",
      "|200005|  Stanley|  Hudson|           Sales|\n",
      "+------+---------+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataDict = [\n",
    "    (200001, \"Michael\", \"Scott\", \"Regional Manager\"),\n",
    "    (200002, \"Dwight\", \"Schrute\", \"Sales\"),\n",
    "    (200003, \"Jim\", \"Halpert\", \"Sales\"),\n",
    "    (200004, \"Phyllis\", \"Lapin\", \"Sales\"),\n",
    "    (200005, \"Stanley\", \"Hudson\", \"Sales\"),\n",
    "    (200006, \"Angela\", \"Martin\", \"Accounting\"),\n",
    "    (200007, \"Kevin\", \"Malone\", \"Accounting\"),\n",
    "    (200008, \"Oscar\", \"Martinez\", \"Accounting\"),\n",
    "    (200009, \"Creed\", \"Bratton\", \"Quality Assurance\"),\n",
    "    (200010, \"Meredith\", \"Palmer\", \"Supplier Relations\"),\n",
    "    (200011, \"Pamela\", \"Beesly\", \"Recepctionist\"),\n",
    "    (200012, \"Kelly\", \"Kapoor\", \"Customer Service\"),\n",
    "    (200013, \"Ryan\", \"Howard\", \"Temp\"),\n",
    "    (200014, \"Toby\", \"Flenderson\", \"Human Resources\"),\n",
    "    (200015, \"Darryl\", \"Philbin\", \"Warehouse Foreman\")\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data = dataDict, schema = [\"empId\", \"firstName\", \"lastName\", \"job\"])\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f782ab6f-1ce2-4fe2-8ff4-85b83c3c4fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+------------------+--------+\n",
      "| empId|firstName|  lastName|               job|    date|\n",
      "+------+---------+----------+------------------+--------+\n",
      "|200001|  Michael|     Scott|  Regional Manager|20230515|\n",
      "|200002|   Dwight|   Schrute|             Sales|20230515|\n",
      "|200003|      Jim|   Halpert|             Sales|20230515|\n",
      "|200004|  Phyllis|     Lapin|             Sales|20230515|\n",
      "|200005|  Stanley|    Hudson|             Sales|20230515|\n",
      "|200006|   Angela|    Martin|        Accounting|20230515|\n",
      "|200007|    Kevin|    Malone|        Accounting|20230515|\n",
      "|200008|    Oscar|  Martinez|        Accounting|20230515|\n",
      "|200009|    Creed|   Bratton| Quality Assurance|20230515|\n",
      "|200010| Meredith|    Palmer|Supplier Relations|20230515|\n",
      "|200011|   Pamela|    Beesly|     Recepctionist|20230515|\n",
      "|200012|    Kelly|    Kapoor|  Customer Service|20230515|\n",
      "|200013|     Ryan|    Howard|              Temp|20230515|\n",
      "|200014|     Toby|Flenderson|   Human Resources|20230515|\n",
      "|200015|   Darryl|   Philbin| Warehouse Foreman|20230515|\n",
      "+------+---------+----------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.withColumn('date', date_format(date_add(current_date(), -3), 'yyyyMMdd').cast('int') ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd68275c-dcd3-46e4-af42-2c67dbcfad02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------------------------------------+\n",
      "|               job|collect_list(concat_ws(_, firstName, lastName))|\n",
      "+------------------+-----------------------------------------------+\n",
      "|             Sales|                           [Jim_Halpert, Phy...|\n",
      "|        Accounting|                           [Oscar_Martinez, ...|\n",
      "| Quality Assurance|                                [Creed_Bratton]|\n",
      "|              Temp|                                  [Ryan_Howard]|\n",
      "|   Human Resources|                              [Toby_Flenderson]|\n",
      "|  Regional Manager|                                [Michael_Scott]|\n",
      "|Supplier Relations|                              [Meredith_Palmer]|\n",
      "|     Recepctionist|                                [Pamela_Beesly]|\n",
      "|  Customer Service|                                 [Kelly_Kapoor]|\n",
      "| Warehouse Foreman|                               [Darryl_Philbin]|\n",
      "+------------------+-----------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.groupBy(col('job')).agg(collect_list(concat_ws('_', col('firstName'),col('lastName')))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd65a27-2f49-4b59-aefb-bd78d9ce6bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
