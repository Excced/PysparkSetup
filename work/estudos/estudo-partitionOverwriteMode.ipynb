{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0c5da85d-2e53-4773-95a2-5e3c3b0e1315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8149bc592a99:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Testando</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fbd32c0fca0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "if 'spark' in locals() or 'spark' in globals():\n",
    "    spark.stop()\n",
    "    \n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Testando\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5d083df2-31c7-46e8-ae81-706728933e7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STATIC'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get(\"spark.sql.sources.partitionOverwriteMode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ab04484c-fecd-4d5d-b5ea-c9ea32600bef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dynamic'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "spark.conf.get(\"spark.sql.sources.partitionOverwriteMode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dc94c917-bdf0-43a2-8622-53ca2130e5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-----------+\n",
      "|namespace|     tableName|isTemporary|\n",
      "+---------+--------------+-----------+\n",
      "|  default|       exemplo|      false|\n",
      "|  default|tb_sales_final|      false|\n",
      "|  default| tb_sales_test|      false|\n",
      "|  default| tbl_employees|      false|\n",
      "+---------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"drop table default.exemplo\")\n",
    "DDL = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS default.exemplo (\n",
    "    col_1 string,\n",
    "    col_2 int\n",
    ")\n",
    "USING PARQUET\n",
    "PARTITIONED BY (anomesdia int)\n",
    "LOCATION 's3a://datalake/exemplo/'\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(DDL)\n",
    "spark.sql(\"show tables in default\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "21127099-f274-4056-ad7a-37827c3b1313",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE default.exemplo (\n",
      "  col_1 STRING,\n",
      "  col_2 INT,\n",
      "  anomesdia INT)\n",
      "USING PARQUET\n",
      "PARTITIONED BY (anomesdia)\n",
      "LOCATION 's3a://datalake/exemplo'\n",
      "TBLPROPERTIES (\n",
      "  'bucketing_version' = '2',\n",
      "  'parquet.compression' = 'SNAPPY')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql('SHOW CREATE TABLE default.exemplo').collect()[0].__getitem__('createtab_stmt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ef1cfbfb-c6ff-4703-b868-f7bb92bb44a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------+\n",
      "|    col_1| col_2|anomesdia|\n",
      "+---------+------+---------+\n",
      "|   banana|200001| 20230412|\n",
      "|abobrinha|200002| 20230412|\n",
      "|   tomate|200003| 20230412|\n",
      "| laranja2|200004| 20230412|\n",
      "|   batata|200005| 20230412|\n",
      "+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDict = [\n",
    "    (\"banana\", 200001, 20230412),\n",
    "    (\"abobrinha\", 200002, 20230412),\n",
    "    (\"tomate\", 200003, 20230412),\n",
    "    (\"laranja2\", 200004, 20230412),\n",
    "    (\"batata\", 200005, 20230412),\n",
    "]\n",
    "\n",
    "df_12 = spark.createDataFrame(data = dataDict, schema = [\"col_1\", \"col_2\", \"anomesdia\"])\n",
    "df_12.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "8dfd71d1-b501-48cc-9213-ba984ca83995",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         partition|\n",
      "+------------------+\n",
      "|anomesdia=20230412|\n",
      "|anomesdia=20230413|\n",
      "+------------------+\n",
      "\n",
      "+---------+------+---------+\n",
      "|    col_1| col_2|anomesdia|\n",
      "+---------+------+---------+\n",
      "|abobrinha|200002| 20230412|\n",
      "|abobrinha|200002| 20230413|\n",
      "| laranja2|200004| 20230412|\n",
      "|  laranja|200004| 20230413|\n",
      "|   banana|200001| 20230412|\n",
      "|   tomate|200003| 20230412|\n",
      "|   batata|200005| 20230412|\n",
      "|   banana|200001| 20230413|\n",
      "|   tomate|200003| 20230413|\n",
      "|   batata|200005| 20230413|\n",
      "+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_12\\\n",
    "    .write\\\n",
    "    .mode('overwrite')\\\n",
    "    .partitionBy('anomesdia')\\\n",
    "    .option(\"path\", \"s3a://datalake/exemplo/\")\\\n",
    "    .saveAsTable('default.exemplo')\n",
    "\n",
    "spark.sql('SHOW PARTITIONS default.exemplo').show()\n",
    "spark.sql(\"SELECT * FROM default.exemplo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "cf01a674-7752-42e7-bc45-6f976a72f83d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------+\n",
      "|    col_1| col_2|anomesdia|\n",
      "+---------+------+---------+\n",
      "|   banana|200001| 20230413|\n",
      "|abobrinha|200002| 20230413|\n",
      "|   tomate|200003| 20230413|\n",
      "|  laranja|200004| 20230413|\n",
      "|   batata|200005| 20230413|\n",
      "+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDict = [\n",
    "    (\"banana\", 200001, 20230413),\n",
    "    (\"abobrinha\", 200002, 20230413),\n",
    "    (\"tomate\", 200003, 20230413),\n",
    "    (\"laranja\", 200004, 20230413),\n",
    "    (\"batata\", 200005, 20230413),\n",
    "]\n",
    "\n",
    "df_13 = spark.createDataFrame(data = dataDict, schema = [\"col_1\", \"col_2\", \"anomesdia\"])\n",
    "df_13.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6fc1a053-4214-4d26-9c2d-7af12f4c4861",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+---------+-----+\n",
      "|    col_1| col_2|anomesdia| data|\n",
      "+---------+------+---------+-----+\n",
      "|   banana|200001| 20230413|20230|\n",
      "|abobrinha|200002| 20230413|20230|\n",
      "|   tomate|200003| 20230413|20230|\n",
      "|  laranja|200004| 20230413|20230|\n",
      "|   batata|200005| 20230413|20230|\n",
      "+---------+------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "df_13.withColumn('data', expr('SUBSTR(anomesdia, 0, 5)')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d4faa627-26a6-4bd3-917a-6c85ef4899c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|         partition|\n",
      "+------------------+\n",
      "|anomesdia=20230412|\n",
      "|anomesdia=20230413|\n",
      "+------------------+\n",
      "\n",
      "+---------+------+---------+\n",
      "|    col_1| col_2|anomesdia|\n",
      "+---------+------+---------+\n",
      "|abobrinha|200002| 20230412|\n",
      "|abobrinha|200002| 20230413|\n",
      "|  laranja|200004| 20230412|\n",
      "|  laranja|200004| 20230413|\n",
      "|   banana|200001| 20230412|\n",
      "|   tomate|200003| 20230412|\n",
      "|   batata|200005| 20230412|\n",
      "|   banana|200001| 20230413|\n",
      "|   tomate|200003| 20230413|\n",
      "|   batata|200005| 20230413|\n",
      "+---------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_13\\\n",
    "    .write\\\n",
    "    .format('parquet')\\\n",
    "    .mode('overwrite')\\\n",
    "    .partitionBy('anomesdia')\\\n",
    "    .option(\"path\", \"s3a://datalake/exemplo/\")\\\n",
    "    .saveAsTable('default.exemplo')\n",
    "\n",
    "\n",
    "spark.sql('SHOW PARTITIONS default.exemplo').show()\n",
    "spark.sql(\"SELECT * FROM default.exemplo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7adf729-6379-4188-b090-6da0761a632c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|partition|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE default.exemplo DROP PARTITION (anomesdia=20230412)\")\n",
    "spark.sql('SHOW PARTITIONS default.exemplo').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7a88e028-ab10-477b-b778-b2abd378fbdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE default.exemplo (\n",
      "  col_1 STRING,\n",
      "  col_2 INT,\n",
      "  anomesdia INT)\n",
      "USING parquet\n",
      "PARTITIONED BY (anomesdia)\n",
      "LOCATION 's3a://datalake/exemplo'\n",
      "TBLPROPERTIES (\n",
      "  'bucketing_version' = '2',\n",
      "  'parquet.compression' = 'SNAPPY',\n",
      "  'transient_lastDdlTime' = '1681303603')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(spark.sql('SHOW create table default.exemplo').collect()[0].__getitem__('createtab_stmt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14d8dddc-4b18-433f-8eb5-9bca90281093",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.sources.partitionOverwriteMode\")\n",
    "spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e94e8d9c-fae4-4171-a59b-93c72103280b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"default.exemplo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "abc16357-4ca0-4361-9f92-b46fdba98f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|anomesdia|\n",
      "+---------+\n",
      "| 20230412|\n",
      "| 20230413|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('anomesdia').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71173b31-34ba-4b2b-9173-5a39ad513fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
