{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bf5fad8-d529-473e-b79b-7d38dd141917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-ee7ac50d-ab26-4183-815d-40b068154526;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.2.0 in central\n",
      "\tfound io.delta#delta-storage;2.2.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      ":: resolution report :: resolve 143ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.2.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.2.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-ee7ac50d-ab26-4183-815d-40b068154526\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/5ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 14:18:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 14:18:07 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6c569a6c6ddd:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Testando Operações DeltaLake</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1571a06140>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if 'spark' in locals() or 'spark' in globals():\n",
    "    spark.stop()\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder \\\n",
    "    .appName(\"Testando Operações DeltaLake\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.2.0\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4c8b79-e053-4ccf-8796-0f86262e013e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%sparksql` not found.\n"
     ]
    }
   ],
   "source": [
    "%%sparksql\n",
    "select * from pokeapi.tbl_silver_pokemons\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57318cc-2af1-455b-90f1-c4828e0ff63a",
   "metadata": {},
   "source": [
    "## Criando dados artificiais\n",
    "\n",
    "Vamos criar uma tabela com dados artificiais com base em um dicionário python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce97ec93-6bcd-4d40-a4cc-076884055025",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+----------------+\n",
      "| empId|firstName|lastName|             job|\n",
      "+------+---------+--------+----------------+\n",
      "|200001|  Michael|   Scott|Regional Manager|\n",
      "|200002|   Dwight| Schrute|           Sales|\n",
      "|200003|      Jim| Halpert|           Sales|\n",
      "|200004|  Phyllis|   Lapin|           Sales|\n",
      "|200005|  Stanley|  Hudson|           Sales|\n",
      "+------+---------+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDict = [\n",
    "    (200001, \"Michael\", \"Scott\", \"Regional Manager\"),\n",
    "    (200002, \"Dwight\", \"Schrute\", \"Sales\"),\n",
    "    (200003, \"Jim\", \"Halpert\", \"Sales\"),\n",
    "    (200004, \"Phyllis\", \"Lapin\", \"Sales\"),\n",
    "    (200005, \"Stanley\", \"Hudson\", \"Sales\"),\n",
    "    (200006, \"Angela\", \"Martin\", \"Accounting\"),\n",
    "    (200007, \"Kevin\", \"Malone\", \"Accounting\"),\n",
    "    (200008, \"Oscar\", \"Martinez\", \"Accounting\"),\n",
    "    (200009, \"Creed\", \"Bratton\", \"Quality Assurance\"),\n",
    "    (200010, \"Meredith\", \"Palmer\", \"Supplier Relations\"),\n",
    "    (200011, \"Pamela\", \"Beesly\", \"Recepctionist\"),\n",
    "    (200012, \"Kelly\", \"Kapoor\", \"Customer Service\"),\n",
    "    (200013, \"Ryan\", \"Howard\", \"Temp\"),\n",
    "    (200014, \"Toby\", \"Flenderson\", \"Human Resources\"),\n",
    "    (200015, \"Darryl\", \"Philbin\", \"Warehouse Foreman\")\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data = dataDict, schema = [\"empId\", \"firstName\", \"lastName\", \"job\"])\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc3f9a4-6e93-4145-a6af-081ca57466b6",
   "metadata": {},
   "source": [
    "## Criando Tabelas Delta no Data Lake + Metastore\n",
    "\n",
    "Criando o database dundermifflin caso ele não exista em nosso metastore e criar a tabela DELTA employees com os dados que criamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d92df22-cc08-45d6-90dd-c7601dd9c3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    namespace|\n",
      "+-------------+\n",
      "|      default|\n",
      "|dundermifflin|\n",
      "|      pokeapi|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS dundermifflin\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ad7f181-541c-441d-8ff0-9ead36906d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/04 12:52:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-----------+\n",
      "|    namespace|        tableName|isTemporary|\n",
      "+-------------+-----------------+-----------+\n",
      "|dundermifflin|        employees|      false|\n",
      "|dundermifflin|employees_updated|      false|\n",
      "+-------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Escrevendo o conteúdo do DataFrame df1 na tabela delta dundermifflin.employees\n",
    "\n",
    "df1.write \\\n",
    "    .option(\"overwriteSchema\", \"true\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .saveAsTable('dundermifflin.employees')\n",
    "spark.sql(\"SHOW TABLES IN dundermifflin\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d349127-539d-4d6b-a4dd-ef66c4f85709",
   "metadata": {},
   "source": [
    "É possível listar as tabelas dentro de um determinado database através da função abaixo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a6f35b-30de-415a-a606-2ba41b0c33f8",
   "metadata": {},
   "source": [
    "Ou através do Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c495f6b4-cb76-464a-9215-3d6c76a113fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+-----------+\n",
      "|    namespace|        tableName|isTemporary|\n",
      "+-------------+-----------------+-----------+\n",
      "|dundermifflin|        employees|      false|\n",
      "|dundermifflin|employees_updated|      false|\n",
      "+-------------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN dundermifflin\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63a7ff-caca-4db8-a67b-726b1655f1c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lendo Tabelas Delta diretamente do S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abda23c1-86fa-414b-b061-8c685b578d89",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+---------------+\n",
      "| empId|firstName|  lastName|            job|\n",
      "+------+---------+----------+---------------+\n",
      "|200014|     Toby|Flenderson|Human Resources|\n",
      "|200008|    Oscar|  Martinez|     Accounting|\n",
      "|200006|   Angela|    Martin|     Accounting|\n",
      "|200007|    Kevin|    Malone|     Accounting|\n",
      "|200002|   Dwight|   Schrute|          Sales|\n",
      "+------+---------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.format(\"delta\").load(\"s3a://warehouse/dundermifflin.db/employees\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8933f6-0105-4ae0-9193-1a0326b52578",
   "metadata": {},
   "source": [
    "# Lendo Tabelas Delta a partir do Metastore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7880b33d-e6e4-4da5-96ba-009203ca9999",
   "metadata": {},
   "source": [
    "### • Utilizando Spark SQL + Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0acb0e-dc9f-4a49-9219-d34a0da3bc70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+---------------+\n",
      "| empId|firstName|  lastName|            job|\n",
      "+------+---------+----------+---------------+\n",
      "|200014|     Toby|Flenderson|Human Resources|\n",
      "|200008|    Oscar|  Martinez|     Accounting|\n",
      "|200006|   Angela|    Martin|     Accounting|\n",
      "|200007|    Kevin|    Malone|     Accounting|\n",
      "|200002|   Dwight|   Schrute|          Sales|\n",
      "+------+---------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.sql(\"SELECT * FROM dundermifflin.employees\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ae341-b7bf-4cc0-851e-f11a1ac17642",
   "metadata": {},
   "source": [
    "### • Utilizando PySpark + Metastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a13b93df-e118-4d03-aae0-85833057c8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+---------------+\n",
      "| empId|firstName|  lastName|            job|\n",
      "+------+---------+----------+---------------+\n",
      "|200014|     Toby|Flenderson|Human Resources|\n",
      "|200008|    Oscar|  Martinez|     Accounting|\n",
      "|200006|   Angela|    Martin|     Accounting|\n",
      "|200007|    Kevin|    Malone|     Accounting|\n",
      "|200002|   Dwight|   Schrute|          Sales|\n",
      "+------+---------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.read.table(\"dundermifflin.employees\")\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5fb4e-619a-4e71-a6d5-ce73afd21589",
   "metadata": {},
   "source": [
    "## Atualizando registros na tabela"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83be29c-9914-44d2-8bec-a1bb85b65bb0",
   "metadata": {},
   "source": [
    "Com tabelas Delta podemos atualizar dados diretamente em uma tabela gravada no deu data lake.\n",
    "\n",
    "Vamos atualizar o campo __job__ do empregado __Dwight Schrute__ para __'Assistant To The Regional Manager'__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c80e72e4-ca09-44a9-82cd-971396b4ecc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbae13-1a6d-4116-ad57-7b9625209ddc",
   "metadata": {},
   "source": [
    "Podemos utilizar PySpark para obter o __empId__ de Dwight e utilizá-lo como chave para alteração do registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae9cf26-59c5-42d9-ad65-13cb80944a0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200002"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwight_id = df1\\\n",
    "    .select(\"empId\")\\\n",
    "    .where(col('firstName') == 'Dwight')\\\n",
    "    .collect()[0]\\\n",
    "    .__getitem__('empId')\n",
    "dwight_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793799d2-17c6-45e1-b921-db35eb1aedfb",
   "metadata": {},
   "source": [
    "Podemos atualizar o dado diretamente na tabela utilizando as classes do pacote delta.tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff39604b-775f-430e-b81e-a51c8f5e0162",
   "metadata": {},
   "source": [
    "• Utilizando um string SQL formatado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "223f8c45-46ec-4f4d-8399-17e03a38a02b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forName(spark, \"dundermifflin.employees\")\n",
    "#deltaTable= DeltaTable.forPath(spark, \"s3a://warehouse/dundermifflin.db/employees\")\n",
    "\n",
    "deltaTable.update(\n",
    "  condition = f\"empId = {dwight_id}\",\n",
    "  set = {\"job\": \"'Assistant To The Regional Manager'\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef54e06-dfde-4cdc-ae97-ea3ef38e0c6a",
   "metadata": {},
   "source": [
    "• Utilizando funções de Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc820352-0264-465c-982b-a39bbc626432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forName(spark, \"dundermifflin.employees\")\n",
    "#deltaTable= DeltaTable.forPath(spark, \"s3a://warehouse/dundermifflin.db/employees\")\n",
    "\n",
    "deltaTable.update(\n",
    "  condition = col('empId') == dwight_id,\n",
    "  set = {\"job\": lit('Assistant To The Regional Manager')}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4ce27cde-bbfc-4f42-8870-546d940d9b75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+---------------------------------+\n",
      "|empId |firstName|lastName  |job                              |\n",
      "+------+---------+----------+---------------------------------+\n",
      "|200002|Dwight   |Schrute   |Assistant To The Regional Manager|\n",
      "|200010|Meredith |Palmer    |Supplier Relations               |\n",
      "|200015|Darryl   |Philbin   |Warehouse Foreman                |\n",
      "|200014|Toby     |Flenderson|Human Resources                  |\n",
      "|200009|Creed    |Bratton   |Quality Assurance                |\n",
      "|200001|Michael  |Scott     |Regional Manager                 |\n",
      "|200012|Kelly    |Kapoor    |Customer Service                 |\n",
      "|200011|Pamela   |Beesly    |Recepctionist                    |\n",
      "|200008|Oscar    |Martinez  |Accounting                       |\n",
      "|200006|Angela   |Martin    |Accounting                       |\n",
      "|200007|Kevin    |Malone    |Accounting                       |\n",
      "|200005|Stanley  |Hudson    |Sales                            |\n",
      "|200004|Phyllis  |Lapin     |Sales                            |\n",
      "|200003|Jim      |Halpert   |Sales                            |\n",
      "|200013|Ryan     |Howard    |Temp                             |\n",
      "+------+---------+----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.table(\"dundermifflin.employees\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57efb8ac-686c-4d7d-b53f-84e2257c91d7",
   "metadata": {},
   "source": [
    "## Deletando Registro na tabela\n",
    "\n",
    "Com tabelas Delta podemos deletar dados diretamente em uma tabela gravada no deu data lake.\n",
    "\n",
    "Vamos deletar o empregado __Ryan Howard__ da tabela __Employees__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ca0be3-f6f5-40b2-b5c6-4ae39def02c9",
   "metadata": {},
   "source": [
    "Podemos utilizar PySpark para obter o __empId__ de Ryan e utilizá-lo como chave para alteração do registro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fe63cec-1169-4fad-9452-95cc5aa0b83d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200013"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ryan_id = df1\\\n",
    "    .select(\"empId\")\\\n",
    "    .where( (col('firstName') == 'Ryan') & (col('lastname') == 'Howard') )\\\n",
    "    .collect()[0]\\\n",
    "    .__getitem__('empId')\n",
    "ryan_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d25d970a-2d30-4e31-9465-d17d882e0918",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+---------------------------------+\n",
      "|empId |firstName|lastName  |job                              |\n",
      "+------+---------+----------+---------------------------------+\n",
      "|200002|Dwight   |Schrute   |Assistant To The Regional Manager|\n",
      "|200010|Meredith |Palmer    |Supplier Relations               |\n",
      "|200015|Darryl   |Philbin   |Warehouse Foreman                |\n",
      "|200014|Toby     |Flenderson|Human Resources                  |\n",
      "|200009|Creed    |Bratton   |Quality Assurance                |\n",
      "|200001|Michael  |Scott     |Regional Manager                 |\n",
      "|200012|Kelly    |Kapoor    |Customer Service                 |\n",
      "|200011|Pamela   |Beesly    |Recepctionist                    |\n",
      "|200008|Oscar    |Martinez  |Accounting                       |\n",
      "|200006|Angela   |Martin    |Accounting                       |\n",
      "|200007|Kevin    |Malone    |Accounting                       |\n",
      "|200005|Stanley  |Hudson    |Sales                            |\n",
      "|200004|Phyllis  |Lapin     |Sales                            |\n",
      "|200003|Jim      |Halpert   |Sales                            |\n",
      "+------+---------+----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable = DeltaTable.forName(spark, \"dundermifflin.employees\")\n",
    "#deltaTable= DeltaTable.forPath(spark, \"s3a://warehouse/dundermifflin.db/employees\")\n",
    "\n",
    "deltaTable.delete(col('empId') == ryan_id)\n",
    "\n",
    "# Pode executar também com string fromatado SQL\n",
    "# deltaTable.delete(f\"empId = {ryan_id}\")\n",
    "\n",
    "spark.read.table(\"dundermifflin.employees\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ecc5c1-3d0e-4199-b980-8a183b431eca",
   "metadata": {},
   "source": [
    "## Executando UPSERTS (Merges)\n",
    "\n",
    "UPSERTS são operações que permitem a escrita e alteração de poucos registros de uma tabela, sem a necessidade de sobrescrever a tabela inteira\n",
    "\n",
    "### UPSERTS = UPDATES + INSERTS. \n",
    "\n",
    "Ao aplicar um UPSERT você insere novos registros e atualiza os registros já existentes em uma tabela alvo, processando apenas os registros necessários.\n",
    "\n",
    "É possível também provocar a deleção de alguns registros ao utilizar essas operações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce29ad-699e-4908-adbf-2adb9ce70e77",
   "metadata": {},
   "source": [
    "Vamos resetar nossa tabela __employees__ para executar todas as alterações novamente usando um UPSERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e10431cd-56f2-4016-8672-22352305c13b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dataDict = [\n",
    "    (200001, \"Michael\", \"Scott\", \"Regional Manager\"),\n",
    "    (200002, \"Dwight\", \"Schrute\", \"Sales\"),\n",
    "    (200003, \"Jim\", \"Halpert\", \"Sales\"),\n",
    "    (200004, \"Phyllis\", \"Lapin\", \"Sales\"),\n",
    "    (200005, \"Stanley\", \"Hudson\", \"Sales\"),\n",
    "    (200006, \"Angela\", \"Martin\", \"Accounting\"),\n",
    "    (200007, \"Kevin\", \"Malone\", \"Accounting\"),\n",
    "    (200008, \"Oscar\", \"Martinez\", \"Accounting\"),\n",
    "    (200009, \"Creed\", \"Bratton\", \"Quality Assurance\"),\n",
    "    (200010, \"Meredith\", \"Palmer\", \"Supplier Relations\"),\n",
    "    (200011, \"Pamela\", \"Beesly\", \"Recepctionist\"),\n",
    "    (200012, \"Kelly\", \"Kapoor\", \"Customer Service\"),\n",
    "    (200013, \"Ryan\", \"Howard\", \"Temp\"),\n",
    "    (200014, \"Toby\", \"Flenderson\", \"Human Resources\"),\n",
    "    (200015, \"Darryl\", \"Philbin\", \"Warehouse Foreman\")\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data = dataDict, schema = [\"empId\", \"firstName\", \"lastName\", \"job\"])\n",
    "\n",
    "df1.write \\\n",
    "    .option(\"overwriteSchema\", \"true\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .saveAsTable(\"dundermifflin.employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02280322-b0d1-47e6-96a2-871112c91f9d",
   "metadata": {},
   "source": [
    "Um UPSERT necessita de dois DataFrames: um __original__ e outro com as __atualizado__\n",
    "\n",
    "Já temos o __original__ armazenado em nosso metastore (tabela employees) , agora precisamos criar um DataFrame com as atualizações que queremos e chamando-o de dfUpdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cac86ac3-ca8d-4c46-a253-0e1ba903021b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+------------------+\n",
      "| empId|firstName|  lastName|               job|\n",
      "+------+---------+----------+------------------+\n",
      "|200010| Meredith|    Palmer|Supplier Relations|\n",
      "|200015|   Darryl|   Philbin| Warehouse Foreman|\n",
      "|200009|    Creed|   Bratton| Quality Assurance|\n",
      "|200014|     Toby|Flenderson|   Human Resources|\n",
      "|200001|  Michael|     Scott|  Regional Manager|\n",
      "|200012|    Kelly|    Kapoor|  Customer Service|\n",
      "|200011|   Pamela|    Beesly|     Recepctionist|\n",
      "|200008|    Oscar|  Martinez|        Accounting|\n",
      "|200006|   Angela|    Martin|        Accounting|\n",
      "|200007|    Kevin|    Malone|        Accounting|\n",
      "|200002|   Dwight|   Schrute|             Sales|\n",
      "|200005|  Stanley|    Hudson|             Sales|\n",
      "|200004|  Phyllis|     Lapin|             Sales|\n",
      "|200003|      Jim|   Halpert|             Sales|\n",
      "|200013|     Ryan|    Howard|              Temp|\n",
      "+------+---------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTableUpdates = DeltaTable.forName(spark, \"dundermifflin.employees\")\n",
    "#deltaTableUpdates = DeltaTable.forPath(spark, \"s3a://warehouse/dundermifflin.db/employees\")\n",
    "\n",
    "dfUpdates = deltaTableUpdates.toDF()\n",
    "dfUpdates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797e4113-3292-429e-b78f-14e4d4f5fc2f",
   "metadata": {},
   "source": [
    "## As alterações que faremos serão:\n",
    "\n",
    "• Alterar os sobrenomes de alguns funcionários\n",
    "\n",
    "• Alterar o cargo de Dwight Schrute para Assistant to the Regional Manager\n",
    "\n",
    "• Criar uma coluna __markedTermination__ que será utilizada como marcador para demissão\n",
    "\n",
    "• Adicionar um funcionário novo na lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "502f7ea4-0cc6-467b-bd7b-88469a4670b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------+---------------------------------+-----------------+\n",
      "|empId |firstName|lastName      |job                              |markedTermination|\n",
      "+------+---------+--------------+---------------------------------+-----------------+\n",
      "|200010|Meredith |Palmer        |Supplier Relations               |false            |\n",
      "|200015|Darryl   |Philbin       |Warehouse Foreman                |false            |\n",
      "|200009|Creed    |Bratton       |Quality Assurance                |false            |\n",
      "|200014|Toby     |Flenderson    |Human Resources                  |false            |\n",
      "|200001|Michael  |Scott         |Regional Manager                 |false            |\n",
      "|200012|Kelly    |Kapoor        |Customer Service                 |false            |\n",
      "|200011|Pamela   |Beesly Halpert|Recepctionist                    |false            |\n",
      "|200008|Oscar    |Martinez      |Accounting                       |false            |\n",
      "|200006|Angela   |Martin        |Accounting                       |false            |\n",
      "|200007|Kevin    |Malone        |Accounting                       |true             |\n",
      "|200002|Dwight   |Schrute       |Assistant to the Regional Manager|false            |\n",
      "|200005|Stanley  |Hudson        |Sales                            |false            |\n",
      "|200004|Phyllis  |Lapin Vance   |Sales                            |false            |\n",
      "|200003|Jim      |Halpert       |Sales                            |false            |\n",
      "|200013|Ryan     |Howard        |Temp                             |false            |\n",
      "+------+---------+--------------+---------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Alterando nomes\n",
    "dfUpdates = dfUpdates.withColumn('lastName',\n",
    "                                 when(col('empId') == 200011, 'Beesly Halpert')\n",
    "                                 .when(col('empId') == 200004, 'Lapin Vance')\n",
    "                                 .otherwise(col('lastName')))\n",
    "# Alterando cargos\n",
    "dfUpdates = dfUpdates.withColumn('job',\\\n",
    "                                 when(col('firstName') == 'Dwight', 'Assistant to the Regional Manager')\\\n",
    "                                 .otherwise(col('job')))\n",
    "# Criando um campo novo\n",
    "dfUpdates = dfUpdates.withColumn('markedTermination',\\\n",
    "                                 when(col('firstName') == 'Kevin', True)\\\n",
    "                                 .otherwise(False))\n",
    "\n",
    "dfUpdates.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24d7f225-89a5-41ae-9d99-9e21297ec703",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------+-----+-----------------+\n",
      "| empId|firstName|lastName|  job|markedTermination|\n",
      "+------+---------+--------+-----+-----------------+\n",
      "|200016|     Andy| Bernard|Sales|            false|\n",
      "+------+---------+--------+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newEmp = [\n",
    "    (200016, \"Andy\", \"Bernard\", \"Sales\", False)\n",
    "]\n",
    "\n",
    "dfnewEmp = spark.createDataFrame(data = newEmp, schema = [\"empId\", \"firstName\", \"lastName\", \"job\", \"markedTermination\"])\n",
    "\n",
    "dfnewEmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "401e686a-9991-415d-be88-2da73ac059e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------+---------------------------------+-----------------+\n",
      "|empId |firstName|lastName      |job                              |markedTermination|\n",
      "+------+---------+--------------+---------------------------------+-----------------+\n",
      "|200010|Meredith |Palmer        |Supplier Relations               |false            |\n",
      "|200015|Darryl   |Philbin       |Warehouse Foreman                |false            |\n",
      "|200009|Creed    |Bratton       |Quality Assurance                |false            |\n",
      "|200014|Toby     |Flenderson    |Human Resources                  |false            |\n",
      "|200001|Michael  |Scott         |Regional Manager                 |false            |\n",
      "|200012|Kelly    |Kapoor        |Customer Service                 |false            |\n",
      "|200011|Pamela   |Beesly Halpert|Recepctionist                    |false            |\n",
      "|200008|Oscar    |Martinez      |Accounting                       |false            |\n",
      "|200006|Angela   |Martin        |Accounting                       |false            |\n",
      "|200007|Kevin    |Malone        |Accounting                       |true             |\n",
      "|200002|Dwight   |Schrute       |Assistant to the Regional Manager|false            |\n",
      "|200005|Stanley  |Hudson        |Sales                            |false            |\n",
      "|200004|Phyllis  |Lapin Vance   |Sales                            |false            |\n",
      "|200003|Jim      |Halpert       |Sales                            |false            |\n",
      "|200013|Ryan     |Howard        |Temp                             |false            |\n",
      "|200016|Andy     |Bernard       |Sales                            |false            |\n",
      "+------+---------+--------------+---------------------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfUpdates = dfUpdates.union(dfnewEmp)\n",
    "\n",
    "dfUpdates.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c62e44-1704-4bc0-af5e-eaf91dccc7da",
   "metadata": {},
   "source": [
    "Essas alterações foram feitas apenas para fins didáticos. Na vida real provavelmente você vai ter mais um batch de alterações armazenado em algum bucket ou tabela e que será necessário processar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "059e7786-7c51-401c-8198-1beb59ad5358",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfUpdates.write \\\n",
    "    .option(\"overwriteSchema\", \"true\")\\\n",
    "    .format(\"delta\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .saveAsTable(\"dundermifflin.employees_updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e611611a-0956-4270-9de9-b6167d052e21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forName(spark, \"dundermifflin.employees\")\n",
    "\n",
    "deltaTableUpdates = DeltaTable.forName(spark, \"dundermifflin.employees_updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "309b6449-0e52-4a88-ad86-e598f46c95c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+--------------+---------------------------------+\n",
      "|empId |firstName|lastName      |job                              |\n",
      "+------+---------+--------------+---------------------------------+\n",
      "|200001|Michael  |Scott         |Regional Manager                 |\n",
      "|200002|Dwight   |Schrute       |Assistant to the Regional Manager|\n",
      "|200003|Jim      |Halpert       |Sales                            |\n",
      "|200004|Phyllis  |Lapin Vance   |Sales                            |\n",
      "|200005|Stanley  |Hudson        |Sales                            |\n",
      "|200006|Angela   |Martin        |Accounting                       |\n",
      "|200008|Oscar    |Martinez      |Accounting                       |\n",
      "|200009|Creed    |Bratton       |Quality Assurance                |\n",
      "|200010|Meredith |Palmer        |Supplier Relations               |\n",
      "|200011|Pamela   |Beesly Halpert|Recepctionist                    |\n",
      "|200012|Kelly    |Kapoor        |Customer Service                 |\n",
      "|200013|Ryan     |Howard        |Temp                             |\n",
      "|200014|Toby     |Flenderson    |Human Resources                  |\n",
      "|200015|Darryl   |Philbin       |Warehouse Foreman                |\n",
      "|200016|Andy     |Bernard       |Sales                            |\n",
      "+------+---------+--------------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "deltaTable.alias('original').merge(dfUpdates.alias('updates'), 'original.empId = updates.empId')\\\n",
    "    .whenMatchedDelete(condition = \"updates.markedTermination = true\")\\\n",
    "    .whenMatchedUpdate(condition = \"updates.markedTermination = false\",\n",
    "                       set = \n",
    "                       {\n",
    "                           \"empId\": \"updates.empId\",\n",
    "                           \"firstName\": \"updates.firstName\",\n",
    "                           \"lastName\": \"updates.lastName\",\n",
    "                           \"job\": \"updates.job\"\n",
    "                       }\n",
    "                      ) \\\n",
    "    .whenNotMatchedInsert(values = \n",
    "                          {\n",
    "                              \"empId\": \"updates.empId\",\n",
    "                              \"firstName\": \"updates.firstName\",\n",
    "                              \"lastName\": \"updates.lastName\",\n",
    "                              \"job\": \"updates.job\"\n",
    "                          }\n",
    "                         )\\\n",
    ".execute()\n",
    "\n",
    "spark.read.table(\"dundermifflin.employees\").show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45baafff-01f8-4302-87cc-ef502f7e5054",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|          timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      5|2023-04-04 12:53:28|  null|    null|               MERGE|{predicate -> (or...|null|    null|     null|          4|  Serializable|        false|{numTargetRowsCop...|        null|Apache-Spark/3.3....|\n",
      "|      4|2023-04-04 12:53:17|  null|    null|CREATE OR REPLACE...|{isManaged -> tru...|null|    null|     null|          3|  Serializable|        false|{numFiles -> 16, ...|        null|Apache-Spark/3.3....|\n",
      "|      3|2023-04-04 12:53:13|  null|    null|              DELETE|{predicate -> [\"(...|null|    null|     null|          2|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      2|2023-04-04 12:53:09|  null|    null|              UPDATE|{predicate -> (em...|null|    null|     null|          1|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      1|2023-04-04 12:53:06|  null|    null|              UPDATE|{predicate -> (em...|null|    null|     null|          0|  Serializable|        false|{numRemovedFiles ...|        null|Apache-Spark/3.3....|\n",
      "|      0|2023-04-04 12:52:54|  null|    null|CREATE OR REPLACE...|{isManaged -> tru...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 16, ...|        null|Apache-Spark/3.3....|\n",
      "+-------+-------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE HISTORY dundermifflin.employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b844674e-26f5-449d-ae16-a64567818d48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------------------------+-----------------------------------+\n",
      "|version|operation                        |engineInfo                         |\n",
      "+-------+---------------------------------+-----------------------------------+\n",
      "|0      |CREATE OR REPLACE TABLE AS SELECT|Apache-Spark/3.3.2 Delta-Lake/2.2.0|\n",
      "|1      |UPDATE                           |Apache-Spark/3.3.2 Delta-Lake/2.2.0|\n",
      "|2      |UPDATE                           |Apache-Spark/3.3.2 Delta-Lake/2.2.0|\n",
      "|3      |DELETE                           |Apache-Spark/3.3.2 Delta-Lake/2.2.0|\n",
      "|4      |CREATE OR REPLACE TABLE AS SELECT|Apache-Spark/3.3.2 Delta-Lake/2.2.0|\n",
      "|5      |MERGE                            |Apache-Spark/3.3.2 Delta-Lake/2.2.0|\n",
      "+-------+---------------------------------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('DESCRIBE HISTORY dundermifflin.employees')\\\n",
    "    .select(['version', 'operation', 'engineInfo'])\\\n",
    "    .orderBy('version')\\\n",
    "    .show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6efb1b28-6a89-44b2-bf30-6ff2c2a6c3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://github.com/delta-io/delta/pull/1255 <- aguardando resolução\n",
    "#spark.sql('SHOW CREATE TABLE dundermifflin.employees').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e19dda19-cea2-44b0-8d12-8605434fcc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ff4f85d6e04298634172ac5d8264e7e9b556b95639fe52ebb9425c4d4cba0c9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
